{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELE435/535 LAB 8: Kernel SVM\n",
    "## Version: 2022\n",
    "\n",
    "## Objectives:  \n",
    "\n",
    "We will use a linear SVM, kernel SVM, and logistic regression to classify MNIST digits into c=10 classes. The best performance among these methods will lower bound what we expect to achive using a Neural Network. Hence it will provide a benchmark for the next round of methods: Multinomial Softmax Regression, a one hidden layer neural network, a Multilayer feedforward neural network, and a convolutional neural network. \n",
    "  \n",
    "The SVM and logistic regression a naturally binary classifiers. But the methods can be used to perform multi-class classification using the one-versus-the-rest method. As the name suggests this trains $c$ binary classifiers, each of which distinguishes one class from the rest. The final classification is made by resolving conflicting classifications (no need to go into the details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from time import time\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel-SVM on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1)** First, import the provided subsets of the MNIST dataset:  \n",
    "MNISTcwtrain1000.npy and  MNISTcwtest100.npy\n",
    "\n",
    "Normalize the scalar data values to the range [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('MNISTcwtrain1000.npy')\n",
    "train_data = train_data.astype(dtype='float64')\n",
    "test_data = np.load('MNISTcwtest100.npy')\n",
    "test_data = test_data.astype(dtype='float64')\n",
    "\n",
    "train_data = train_data/255.0\n",
    "test_data = test_data/255.0\n",
    "print('training data: ', train_data.shape)\n",
    "print('testing data: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2)** The SVM can be used to classify multiclass data. One-versus-the-rest is the default when you call Scikit to learn an SVM with multi-class labels.\n",
    "\n",
    "Train a one-versus-rest SVM using a linear kernel and C=0.1.   \n",
    "Report classification accuracy on the training and testing data. (Use sklearn's built-in commands for training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided\n",
    "\n",
    "from sklearn import svm\n",
    "start = time()\n",
    "#-----------------------------------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------\n",
    "# This code is provided\n",
    "end = time()\n",
    "print('Estimated running time:' + str(datetime.timedelta(seconds=end - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3)** Now, do the same using an SVM with 'rbf' (Gaussian) kernel. Search over C in the interval [0.005,0.1] and 'gamma' in the interval [0.005, 0.1] and report the best test accuracy (use sklearn's built-in commands). Hint: In order to get a feeling for selecting an appropriate value for gamma, take a look at http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided\n",
    "start = time()\n",
    "#--------------------------------------------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# This code is provided\n",
    "end = time()\n",
    "print('Estimated running time:' + str(datetime.timedelta(seconds=end - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3)** Now, do the same using l2 regularized logistic regression. For multi-class data, scikit learn defaults to one-versus-the-rest classification. The regularization parameter $C$ plays the role of $1/\\lambda.$ Smaller values of $C$ mean stronger regularization. Search over three or fours values in the interval [0.01, 1] to find the best testing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is provided\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time()\n",
    "#--------------------------------------------\n",
    "# Your code here\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# This code is provided\n",
    "end = time()\n",
    "print('Estimated running time:' + str(datetime.timedelta(seconds=end - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4)** With what accuracy can the best of the above clasifiers predict the classes of the 1,000 test images? This is the benchmark to beat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
