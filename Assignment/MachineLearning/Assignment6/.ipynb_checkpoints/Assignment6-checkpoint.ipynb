{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELE 435/535 Homework 6\n",
    "\n",
    "## Unconstrained Optimization Using Gradient Descent\n",
    "\n",
    "In this homework, we will examine the minimization of functions using gradient descent. We examine the convergence rate for convex and strongly convex functions. We also note that the convergence rate depends on the characteristics of the problem.\n",
    "\n",
    "Credits: Parts 1 and 2 of this assignment have been adapted from practicals by Francis Bach, Alexandre d'Aspremont, Pierre Gaillard and Aude Genevay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Let $f\\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be differentiable with gradient $\\nabla f(x)$. Gradient descent attempts to find a local minimum of $f$ using the iterative algorithm:\n",
    "$$\n",
    "x_{t+1} = x_t -\\gamma \\nabla f(x_t),\\quad x_0\\in \\mathbb{R^n}.\n",
    "$$\n",
    "\n",
    "In order to ensure convergence to a local minimum the fixed step size $\\gamma$ can't be too big. The standard fixed step size is $\\gamma = 1/L$  where $L$ is a uniform upper bound on the largest eigenvlaue of $\\nabla^2f(x)$.\n",
    "    \n",
    "### Linear Convergence\n",
    "\n",
    "When the rate of convergence of an iterative optimization algorithm satisfies \n",
    "\n",
    "$$\n",
    "|f(x_t)-f(x^\\star)| \\leq C \\alpha ^t\n",
    "$$\n",
    "\n",
    "for some constant $0< \\alpha <1$, the error $|f(x_t)-f(x^\\star)|$ converges to $0$ exponentially in $t$. By taking logs of both sides you see that \n",
    "\n",
    "$$\n",
    "\\log (|f(x_t)-f(x^\\star)|)\\leq t \\log(\\alpha) + \\log (C).\n",
    "$$\n",
    "\n",
    "This is also termed \"linear convergence\".\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "We will examine the ridge regression problem: \n",
    "\n",
    "$$\\min_{x \\in \\mathbb{R}^n}\\  f(x) = \\frac{1}{2m}\\|y - F x\\|^2+\\frac{\\lambda}{2} \\|x\\|^2$$\n",
    "\n",
    "Here $F \\in \\mathbb{R}^{m \\times n}$ is a given matrix and $y \\in \\mathbb{R}^m$ is a given vector. The constant $\\lambda$ is a hyperparameter that weights the relative importance of the second term versus the first in $f(x)$. The function to be minmized is a quadratic function of $x$.\n",
    "\n",
    "We have selected this problem as a testbed since it has a known solution. The optimal $x^\\star$ for the ridge regression problem is\n",
    "\n",
    "$$\n",
    "x^\\star = (F^T F + m\\lambda I)^{-1}F^T y.\n",
    "$$\n",
    "\n",
    "What we want to explore the convergence rate of gradient descent to the known solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Starting the Computational Exercises\n",
    "\n",
    "You will need to find: $\\nabla f(x)$ and $\\nabla^2 f(x)$ for the ridge regression problem. Also consider whether $f(x)$ is strongly convex. If so, determine the maximum value of $c$ for which it is $c$-strongly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: $\\nabla f(x) = \\left (\\frac{1}{m} F^TF + \\lambda I_n \\right ) x - \\frac{1}{m} F^T y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: $\\nabla^2f(x) = \\frac{1}{m}F^T F + \\lambda I_n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: $f(x)$ is strongly convex since $f(x) -\\frac{\\lambda}{2}\\|x\\|_2^2$ is convex. It is hence also strictly convex and convex. To find the largest $c$ for which it is $c$-strongly convex expand $f(x)$ to find the quadratic term. Subtractinhg $\\frac{c}{2} x^Tx$ from this term yields\n",
    "    $$\n",
    "    x^T \\left (\\frac{1}{2m} F^TF + \\frac{\\lambda}{2} I -\\frac{c}{2} I\\right ) x\\ +\\ ...\n",
    "    $$\n",
    "To ensure the result is convex we need the quadratic term to be PSD. Hence\n",
    "    $$\n",
    "    c \\leq \\lambda + \\frac{1}{m} \\lambda_{\\min}(F^TF).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm-up: strongly convex versus strictly convex\n",
    "\n",
    "### Two scalar functions \n",
    "\n",
    "(a) Write a python function to compute and return $f_1(x)=x^2$ and $\\nabla f_1(x)$.\n",
    "\n",
    "(b) Write python function to compute and return $f_2(x)=x^4$ and $\\nabla f_2(x)$.\n",
    "\n",
    "(c) Code a gradient descent algorithm that calls the appropriate functions defined above and computes the minimum of the function. Use a step size $\\gamma = 0.1$, a maximum of $200$ interations, and stopping criterion $|\\nabla f(x)|< \\epsilon =10^{-3}$.\n",
    "\n",
    "(d) Plot $|x_t-x^\\star|$ versus the number of iterations $t$, where $x_t$ is the $t$-th iterate of gradient descent and $x^\\star$ is the known solution. To display the convergence speed of the algorithms, plot in a logarithmic scale. For this you may find the Python functions `semilogx, semilogy, loglog` useful. Display convergence plots for both functions on one graph. How and why are the plots different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective function x^2\n",
    "\n",
    "\n",
    "# define the objective function x^4\n",
    "\n",
    "\n",
    "\n",
    "# GD Parameters\n",
    "gamma = 1/10\n",
    "n_iter_max = 100\n",
    "eps = 10**(-3)  # stopping criterion\n",
    "\n",
    "# Initialisation\n",
    "x_init = 1.0\n",
    "\n",
    "# f_star\n",
    "f_star = 1.0\n",
    "\n",
    "# Gradient descent\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ridge Regession\n",
    "\n",
    "### Step 1: Generate F and y\n",
    "\n",
    "(a) Generate a random matrix $F \\in \\mathbb{R}^{m \\times n}$ of size $m=50$ and $n=60$ where each row of $F$ belongs to $[0,1]^n$. The numpy cammand `np.random.rand` may be useful. Note that $F$ will have linearly dependent columns. Why?\n",
    "\n",
    "(b) Model $y$ as $Fx + w$ where $x\\in \\mathbb{R}^n$ and $w$ is a normally distributed noise vector in $\\mathbb{R}^m$. Generate $x \\in [0,1]^n$. Then generate a target vector $y \\in \\mathbb{R}^m$. The numpy command `random.randn` may be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: The number of columns in $F$ is greater than the number of rows. Hence the rank of $F$ is at most the number of rows. So the columns must be linearly dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 60 # dimension of x\n",
    "m = 50  # number of data points\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Numerically compute the solution of ridge regression\n",
    "\n",
    "(a) Numerically compute the largest eigenvalue and smallest eigenvalue of $\\nabla^2 f(x) = P = \\frac{1}{m}F^T F + \\lambda I$. These will be used to set the constant step size in gradient descent and to bound the rate of convergence.\n",
    "\n",
    "In addition, compute and display the condition number of the matrix:\n",
    "$$\\frac{\\lambda_{\\max}(P)}{\\lambda_{\\min}(P)}.$$\n",
    "A very large condition number is a cautionary warning. Investigate what happens for small $\\lambda$ values (say, $\\lambda=0.01$). Report your observations and interpretation below.\n",
    "\n",
    "(b) Now compute $x^\\star$ and $f(x^\\star)$ numerically. It's good practice to avoid computing a matrix inverse. Instead solve a set of linear equations. See the numpy command `linalg.solve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS: The matix $F$ has more columns than rows and hence $F^TF$ must be singular. Adding $\\lambda I_n$ moves the matrix away from singularity, the larger the value of $\\lambda$ the more distant it is from being singular. As $\\lambda\\rightarrow 0$ the matrix $M$ will approach singularity and $\\kappa(M) \\uparrow \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1.0         # regularization parameter (lambda)\n",
    "\n",
    "# component terms\n",
    "\n",
    "# Note that P is a symmetric matrix\n",
    "\n",
    "# solve ridge regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Solve ridge regression using gradient descent\n",
    "\n",
    "Now that we know the solution $x^\\star$ and the largest and smallest eigenvalues of $P$ and we can explore the convergence of gradient descent with both constant stepsize and variable step size.\n",
    "\n",
    "(a) Put your code from the above steps together into one new code block to implement gradient descent using the standard constant step-size to numerically find the vector $x_*$ that minimizes the ridge regression function and the minimum value of the function. \n",
    "\n",
    "* Use $\\lambda = 1.0$.\n",
    "\n",
    "* Stopping criterion: $\\|\\nabla f(x)\\|_2 < \\epsilon = 10^{-3}$. The numpy command `linalg.norm` may be useful. \n",
    "\n",
    "* Set the constant step size $\\gamma$ using largest eigenvalue of $\\nabla^2f(x)$. \n",
    "\n",
    "(b) Display your results by plotting $\\|x_t-x^\\star\\|_2$ versus the number of iterations $t$, where $x_t$ is the $t$-th iterate of gradient descent and $x^\\star$ is the pre-computed ridge regression solution. The convergence speed of algorithms is displayed by plotting in a logarithmic scale. For this you may find the Python functions `semilogx, semilogy, loglog` useful. Your plot should be a straight line.\n",
    "\n",
    "(c) For a quadratic objective is easy to solve $\\gamma_t =\\arg\\min_\\gamma f(x_t-\\gamma \\nabla f(x_t) )$ to find the optimal step size at step $t.$ The solution gives\n",
    "$$\n",
    "\\gamma_t = \\frac{g_t^Tg_t}{g_t^T P g_t}\n",
    "$$\n",
    "where $g_t = \\nabla f(x_t)$ and $P$ is the symmetric PD matrix in the quadradtic.\n",
    "Add to your code an implementation of gradient descent using the optimal variable step size (given above) to find the minimum and minimum value of the ridge regression function. Use the same parameters as in part (a).\n",
    "To aid comparison, plot your convergence results on the same graph produced in part (b).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to specify n, m, and select F and y\n",
    "\n",
    "n = 60 # dimension of x\n",
    "m = 50  # number of data points\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "\n",
    "# Your definition of python function to return ridge function value and gradient\n",
    "\n",
    "\n",
    "# regularization parameter (lambda)\n",
    "reg = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Logistic Regression\n",
    "\n",
    "Now you will write code to execute gradient descent with a constant step size to train a logistic regression classifier by minimizing the cross-entropy loss.\n",
    "\n",
    "(a) First write code to generate $nx=100$ random Gaussian examples in $\\mathbb{R}^2$ from two classes with means $\\mu_1,$ and $\\mu_2.$ You can asume the Gaussian densities are circularly symmetric.\n",
    "One way to do  this is store the class 0 data in an array X0, and class 1 data in an array X1. You will also need to have corrsponding arrays the labels.\n",
    "\n",
    "You then then augment the X0, X1 arrays to account for the offset parameter. Finall you can pack these togther using np.hstack() and np.vstack() to get two final arrays one the augmented X and one for the labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data from two Gaussian sources woth labels 0 and 1\n",
    "\n",
    "nx = 100        #number of examples per class\n",
    "mu0=1.0\n",
    "sig0=0.8\n",
    "mu1= -1.0\n",
    "sig1=0.5\n",
    "#---------------------------------------------\n",
    "\n",
    "#Your Code Here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Write three functions:  \n",
    "(1) Function one takes as input a scalar z and returns $\\sigma(z)=\\frac{1}{1+e^{-z}}$.  \n",
    "(2) Function two takes as input the data X, y and the parameter $w \\in \\mathbb{R}^3$ and the total cross-entropy loss of $w$.  \n",
    "(3) Function three takes the same inputs as function two and returns the gradient of the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Now write code to execute gradient descent to minimize the cross-entropy loss by iterative updating the parameter vector $w\\in \\mathbb{R}^3$.  \n",
    "Plot your results on two plots:  \n",
    "(a) In the first plot show the total loss $J(w)$ versus the interation index $t$. Use a log axis for the total loss.  \n",
    "(b) Plot the training data (color coded by class), the initial value of the vector component of $w$ in $\\mathbb{R}^2$, the final version of this vector, and the final decision hyperplane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
